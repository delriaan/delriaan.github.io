---
title: Data Exploration
output: html_document
editor_options: 
  chunk_output_type: console
---

:::{.panel-tabset}

## 1. Initialize

This section executes global tasks that set up the workspace are executed:

`r htmltools::span(style="font-weight:bold", "Load Libraries")`

This code block sets global parameters for the session and loads an initial set of libraries. Custom libraries are hosted on Github at [github.com/delriaan](https://github.com/delriaan?tab=repositories) and include the following:

-   book.of.utilities
-   book.of.features
-   book.of.workflow
-   smart.data
-   DBOE
-   event.vectors
-   architect

```{r}
#| label: INITIALIZE
#| message: true
#| warning: false
#| echo: false

# :: Global Parameters ----
options(
  params = { list(
      proj_name = "COVID-19 NURSING HOME DATA"
      , dsn = "IMPERIALTOWER"
      , req_libs = rlang::exprs(DBI, magrittr, docstring, plotly, stringi, openxlsx, lubridate
                                , askpass, keyring, xml2, httr2, jsonlite, lexicon, cachem
                                , memoise, furrr, doFuture, future.callr, tictoc, pdftools
                                , arrow, duckdb)
      , opt_libs = rlang::exprs(igraph, kableExtra, text2vec, LDAvis, isofor, glmnetUtils, lmerTest)
      , git_libs = rlang::exprs(book.of.utilities,book.of.features,book.of.workflow,smart.data,DBOE,event.vectors, architect)
      , new_data_date = {(lubridate::today() - lubridate::years(1)) |> lubridate::rollback() + lubridate::days(1)}
      , sub_dirs = list(globals = "globals", data = "data", cache = "r_session_cache")
      , ext_data_dir = Sys.getenv("ONEDRIVE") |> 
          paste0("/Ext Data") |>
          dir(pattern = "CMS", full.names = TRUE, include.dirs = TRUE)
      , covid19_data.url = "https://data.cms.gov/data-api/v1/dataset/137f90cb-ac53-4b3d-8358-e65cf64e03d3/data"
      , covid19_data_dict.url = "https://data.cms.gov/sites/default/files/2023-08/COVID-19%20Nursing%20Home%20Data%20Dictionary.pdf"
      )
    }
  , future.globals.maxSize = c(10 * 1024 * 10E6)
  , cache_sz  = magrittr::set_attr(10 * (1024^3), "desc", "Disk-based cache size in Gb")
  );

# :: Load Libraries ----
library(book.of.workflow)
spsUtil::quiet(getOption("params")[c("req_libs", "git_libs")] |> purrr::walk(\(i) load_unloaded(!!!i)));

# :: Global Objects ----
c("META", "workflow") |> 
  walk(\(x) assign(x, new.env(), envir = .GlobalEnv));

# `caches` allows for easy storage of workspace objects to disk using `cachem::cache_disk()`
caches <- { cachem::cache_disk(
  dir = "r_session_cache"
  , max_n = Inf
  , logfile = "cache_log"
  , max_size = getOption("cache_sz") |> as.vector()
  )}

# `refer.to` is a function that returns the environment of an object if it 
#   exists in the search path or in the workspace
refer.to <- \(i){
    if (is.environment(i)){ i } else if (i %in% search()){ 
      as.environment(i) 
    } else { NULL }
  }

# `unzip.iter` is a function that iterates over `obs_data_files` and unzips 
#   each file to its parent directory
unzip.iter <- \(from, to, clean = FALSE){
  require(magrittr)
  # browser()
  # Capture the parent directory of `from` ----
  zip_dir <- dirname(from);
  
  # Create the name of the CSV file ----
  csv_file <- (utils::unzip(zipfile = from, list = TRUE))[, 1] |>
        purrr::keep(\(i) stringi::stri_detect_fixed(str = i, pattern = "COVID"));
  
  if (rlang::is_empty(csv_file)){ 
    return(FALSE) 
  } else { 
    csv_file <- paste(zip_dir, csv_file, sep = "/");
  }
  
  # browser()
  
  # Create the name of the parquet file ----
  if (!dir.exists(to)){ dir.create(to, recursive = TRUE) }
  if (clean){ rlang::inject(file.remove(!!!dir(to, pattern = "parquet", full.names = TRUE))) }
  
  parquet_file <- paste(
    to
    , stringi::stri_replace_last_fixed(
        str = csv_file
        , pattern = ".csv"
        , replacement = ".parquet"
        , vectorize_all = FALSE
        ) |>
      stringi::stri_split_fixed("/", simplify = TRUE) %>% 
      .[, ncol(.)] |>
      stringi::stri_trans_tolower() |>
      stringi::stri_replace_all_regex("[_[:space:]]", "-", vectorize_all = FALSE)
    , sep = "/"
    );
  
  # Check if the parquet file already exists ----
  if (file.exists(parquet_file)){
    # If so, do nothing
    message(glue::glue("{parquet_file} already exists: skipping ..."), sep = "\n");
    return(NULL);
  } else {
    # Otherwise, unzip `from` to `exdir` ====
    utils::unzip(zipfile = from, exdir = zip_dir, overwrite = !TRUE); 
    
    # Send the user a message that `from` is being unzipped to `zip_dir` ====
    cat(glue::glue("Unzipping {from} to {zip_dir}"), sep = "\n");
  
    if (rlang::is_empty(csv_file)){ 
      return() 
    } else {
      # Choose the largest CSV file ====
      csv_file <- csv_file[(\(m) m$size == max(m$size))(file.info(csv_file))];
    
      cat(glue::glue("Converting {csv_file} to {parquet_file}"), sep = "\n");
      
      # Convert the largest CSV file to parquet ====
      arrow::read_csv_arrow(csv_file) |>
        arrow::write_parquet(parquet_file);
      
      # Check if the parquet file was successfully written ====
      if (file.exists(parquet_file)){ 
        # Delete the unzipped CSV file
        cat(glue::glue("Successfully wrote {parquet_file}"), sep = "\n");
        unlink(csv_file);
        cat(glue::glue("Deleted {csv_file}"), sep = "\n");
        return(TRUE);
      } else {
        # Failed to write parquet file
        cat(glue::glue("Failed to write {parquet_file}"), sep = "\n");
        return(FALSE);
      }
    }
  }
}

# :: Environment Integrity Criteria ----
.GlobalEnv %must.have% !!c("db_conns", "caches", "refer.to", ".file_root", ".image_file_root", "dbms.monitor");

.GlobalEnv %must.have% "-c";

workflow %must.have% "ez_pass";
workflow$ez_pass <- list(
    sql = kr_key(service = "SQLServer", username = "imperial_agent", keyring = "local")
    , m365 = kr_key(service = "M365", username = Sys.getenv("ONEDRIVE_URL"), keyring = "api_keys")
    );

check.env(refer.to("workflow"));

# :: File Integrity Checks ----
if (interactive()){ 
  	message("Interactive session detected ...");
  	setwd(dirname(rstudioapi::getSourceEditorContext()$path));
  } else { 
    message("Non-interactive session detected ...");
  }

message(paste0("Current working directory is", getwd()));

.file_root <- getwd();
.image_file_root <- paste(.file_root, getOption("params")$sub_dirs$data, sep = "/");

c("_SANDBOX") |> walk(\(x){
		if (!file.exists(paste0(.file_root, "/", x, ".R"))){
		  cat(paste0("# ", x), file = paste0(.file_root, "/",  x, ".R"), append = FALSE) 
		}
  });

walk(getOption("params")$sub_dirs, \(x){
  if (!dir.exists(x)) dir.create(x); 
});

# :: Source setup files ----
dir(pattern = "0[1-4].+R$", full.names = TRUE) |> 
  sprintf(fmt = "message(\"%s\"); source(\"%1$s\"); ") |>
	rlang::parse_exprs() |> walk(eval);

# Custom GitHub resources
invisible(Sys.getenv("GIT_REPOS") |> 
  paste0("/resources/R") |> 
  dir(pattern = "(cache|calc|lazy|model|list|plotly|nlp|ident).+R$", recursive = TRUE, full.names = TRUE) |>
  map(source));

dir(getOption("params")$sub_dirs$globals, full.names = TRUE) |> walk(source);

```

## 2. Load Existing Data

```{r}
#| label: LOAD_EXISTING_DATA
#| code-summary: Load existing data and make local database connections
#| message: true
#| warning: false
#| echo: true
#| cache: true

# :: Database Connections ====
if (!hasName(globalenv(), "db_conns")){
  assign("db_conns", list());
  
  db_conns$LOCAL <- dbConnect(
    drv = odbc::odbc()
    , dsn = getOption("params")$dsn
    , UID = workflow$ez_pass$sql@username
    , PWD = workflow$ez_pass$sql@key()
    );
  
  duck_drv <- duckdb();
  db_conns$DUCK <- dbConnect(drv = duck_drv)
}

# :: Database Metadata ====
tic("$get.metadata()");
dbms.monitor <- DBOE$new()$get.metadata(!!!db_conns[1], chatty = !TRUE);
toc(log = TRUE);

tic("Load Saved Data")
message("Load cached data");
cache_load(cache = caches, pattern = "^(global|base|arch|temp|meta)");
toc(log = TRUE)


check.env(.GlobalEnv);
```

## 3. Data Dictionary 

A data dictionary is provided in PDF format.  Using the PDF Tables service, the PDF was converted into CSV for easier processing:

```{r}
#| label: DATA_DICTIONARY
#| code_summary: Read in and transform the data dictionary
#| echo: false
#| warning: false

# Read the data dictionary into R
# As a reminder, `define()` comes from the package `architect` hosted on GitHub.
data_dict <- { define(
  # Set at the data argument for the `define()` function
  data = dir(
      path = getOption("params")$ext_data_dir
      , pattern = "Dictionary.csv"
      , recursive = TRUE
      , full.names = TRUE
      ) |>
    readr::read_csv(
      quote = "\""
      , col_types = "ccc"
      , col_names = c("term", "desc", "type")
      , skip = 1
      )
  # Remove the row number column and the last three rows which do not contain terms
  , .SD[-(.N - c(0:2))][, !"rn"]
  # Column 'Type' is not populated for each row: I'll use function 
  #   `book.of.utilities::count.cycles()` to define row groups within which
  #   column values will be collected into a single row-column vector
  , row_grp = count.cycles(!is.na(type), reset = FALSE)
  , map(.SD, \(i) paste(na.omit(i), collapse = " "))  ~ row_grp
  # Remove the row group column and return unique rows
  , unique(.SD[, !"row_grp"])
  # Add a column for the abbreviated term
  , abbr_term = stri_extract_all_regex(term, "[A-Z]+[a-z]") |>
      map_chr(\(i) i |> stri_join(collapse = "_"))
  , ~term + type + desc + abbr_term
  )}

# Render the data dictionary using `DT::datatable()`
data_dict.viz <- { data_dict |>
  # Modify column 'abbr_term' to display the full term as a tooltip
  modify_at("abbr_term", \(at){
    map_chr(at, \(i){
        long <- paste(i, collapse = "_");
        short <- ifelse(
        stri_length(long) > 20L
        , stri_sub(long, length = 20L) |> paste0(" ...")
        , long
        );
        glue::glue("<span title='{long}'>{short}</span>") |>
        htmltools::HTML()
      })
    }) |>
  DT::datatable(
    rownames = FALSE
    , fillContainer = FALSE
    , escape = FALSE
    , colnames = c("Term", "Type", "Description", "Short Term")
    , filter = "none"
    , class = c("compact", "wrap")
    , extensions = c("Buttons", "Scroller", "Responsive")[-3]
    , options = list(
        dom = "B<tip>"
        , buttons = c("csv", "excel", "pdf", "print")
        , scroller = TRUE
        , scrollY = 400
        , search = list(regex = TRUE)
        , autoWidth = !TRUE
        , columnDefs = list(list(targets = "_all", width = "25%"))
      )
    ) |>
  DT::formatStyle(columns = 1, width = "25%", whiteSpace = "normal", fontSize = ".9em") |>
  DT::formatStyle(columns = 2, width = "25%", whiteSpace = "normal", fontSize = ".9em") |>
  DT::formatStyle(columns = 3, width = "25%", whiteSpace = "normal", fontSize = ".8em") |>
  DT::formatStyle(columns = 4, width = "25%", whiteSpace = "normal", fontSize = ".8em")
}
```

## 4. Data Exploration

```{r}
#| label: DATA_EXPLORATION
#| code_summary: Explore the data
#| echo: true
#| warning: false
#| message: false
#| cache: true

if (!"global_obs_summary" %in% caches$keys()){
  # Read parquet files into the workspace
  obs_data <- open_dataset(
    sources = getOption("params")$ext_data_dir |> paste0("/data files/parquet")
    , hive_style = FALSE
    , partitioning = c("Year", "Month")
    );

  if (!interactive()){ 
    stop("`obs_summary` not found in cache: code must be run interactively in order to generate.")
  }
  
  # Because the dataset is large, I used repeated sampling to generate a summary of 
  #   descriptive statistics for the dataset. This is done by sampling 100 observations
  #   from the dataset 30 times. The results are then combined and summarized. 
  # Columns that are all NA or integer64 are ignored, and rows with less than 60% valid
  #   values are also ignored.
  obs_summary <- map2(1:30, 100, slowly(\(i, n){
      # obs_data.query |> 
      obs_data |> 
        dplyr::slice_sample(n = n) |>
        dplyr::mutate(sample_sz = n, iteration = i) |>
        dplyr::collect() |>
        dplyr::distinct() |>
        dplyr::collect() |>
        setDT()
      }, rate_delay(0.5))) |> 
    rbindlist() |>
    discard(
      # Ignore columns that are all NA or integer64
      \(i) bit64::is.integer64(i) | all(is.na(i))
      ) |>
    setcolorder(c("Year", "Month"));
  
  # The data is saved to a cache file for reuse in the project
  cache_prep(obs_summary) |> cache_save(caches);
  
  gc()
} else {
  cache_load(caches, "obs_summary");
}

obs_summary.viz <- as.data.table(obs_summary)[
  , mget(c(grep("iteration|sample_sz|Zip", names(.SD), invert = TRUE, value = TRUE)))
  ] |> 
  # Calculate summary statistics
  summarytools::descr(transpose = TRUE) |> 
  (\(i){
    # Replace specific row names with links to the definition of the term
    rownames(i) <- stri_replace_all_regex(
      rownames(i)
      , pattern = "(Contraindication[s])"
      , replacement = "<span style=\"color:blue; text-decoration:underline; \" title=\"Contraindication is a medical term used for a specific situation or factor that makes a procedure or course of treatment inadvisable because it may be harmful to a person. (source: https://www.drugs.com/medical-answers/contraindication-mean-3561035/)\">$1</span>"
      , vectorize_all = FALSE
      ); 
    
    # Replace specific column names with links to the definition of the term
    colnames(i) <- stri_replace_all_fixed(
      colnames(i)
      , pattern = c("CV", "MAD")
      , replacement = c(
          "<span style=\"color:blue;\" title=\"Standard deviation divided by the mean (source: https://statisticsbyjim.com/basics/coefficient-variation/)\">CV</span>"
          , "<span style=\"color:blue;\" title=\"The median of the absolute deviations from the data's median (source: https://en.wikipedia.org/wiki/Median_absolute_deviation)\">MAD</span>"
          )
      , vectorize_all = FALSE
      );
    
    # Return the modified object
    i
  })() |>
  # Retain rows where the percent valid is greater than 50%
  dplyr::filter(Pct.Valid > 60) |>
  # Transform the results into a web table using DT
  DT::datatable(
    extensions = c("Buttons", "FixedHeader", "FixedColumns")
    , escape = FALSE
    , options = { list(
        dom = "Bfrtip"
        , buttons = c("copy", "csv", "excel", "pdf", "print")
        , scrollX = TRUE
        , pageLength = 5
        , fixedHeader = TRUE
        , fixedColumns = list(leftColumns = 1)
        , search = list(regex = TRUE, caseInsensitive = TRUE)
        , columnDefs = list(
            list(
              targets = "_all"
              , # round decimals to four places
                render = DT::JS(
                  "function(data, type, row, meta) {",
                  "return type === 'display' && typeof data === 'number' ?",
                  "data.toFixed(4) : data;",
                  "}"
                )
            )
          )
        )}
    , caption = "Summary of Sampled Observations"
    ) |>
  # Append additional content to the generated HTML object
  htmlwidgets::appendContent(
    htmltools::tags$ol(
      htmltools::tags$li("Show above is a summary of descriptive statistics for a sample of observations from the dataset.")
      , htmltools::tags$li("The sample size is 100 observations and the sample is drawn randomly 30 times. Only rows with greater than or equal to 60% valid values and columns that are not all NA or integer64 are shown.")
      , htmltools::tags$li("The table is interactive and can be searched, sorted, and filtered.")
    )
  );

obs_summary.viz

```

:::
