[
  {
    "objectID": "CMS_C19_NH_ETL.html",
    "href": "CMS_C19_NH_ETL.html",
    "title": "ETL and Data Engineering",
    "section": "",
    "text": "1. Initialize2. Load Existing Data3. Get External DataAppendix\n\n\nThis section executes global tasks that set up the workspace are executed:\nLoad Libraries\nThis code block sets global parameters for the session and loads an initial set of libraries. Custom libraries are hosted on Github at github.com/delriaan and include the following:\n\nbook.of.utilities\nbook.of.features\nbook.of.workflow\nsmart.data\nDBOE\nevent.vectors\narchitect\n\n\n\nNo required values set: exiting ..;\n\n\n\n\n\n\nLoad existing data and make local database connections\n# :: Database Connections ====\nif (!hasName(globalenv(), \"db_conns\")){\n  assign(\"db_conns\", list());\n  \n  db_conns$LOCAL &lt;- dbConnect(\n    drv = odbc::odbc()\n    , dsn = getOption(\"params\")$dsn\n    , UID = workflow$ez_pass$sql@username\n    , PWD = workflow$ez_pass$sql@key()\n    );\n  \n  duck_drv &lt;- duckdb();\n  db_conns$DUCK &lt;- dbConnect(drv = duck_drv)\n}\n\n# :: Database Metadata ====\ntic(\"$get.metadata()\");\ndbms.monitor &lt;- DBOE$new()$get.metadata(!!!db_conns[1], chatty = !TRUE);\ntoc(log = TRUE);\n\n\n$get.metadata(): 0.35 sec elapsed\n\n\nLoad existing data and make local database connections\ntic(\"Load Saved Data\")\nmessage(\"Load cached data\");\ncache_load(cache = caches, pattern = \"^(global|base|arch|temp|meta)\");\ntoc(log = TRUE)\n\n\nLoad Saved Data: 0.06 sec elapsed\n\n\nLoad existing data and make local database connections\ncheck.env(.GlobalEnv);\n\n\n\n\nThe CMS data was downloaded as separate files for a total compressed size of over 10GB. Each file contained one or more CSV documents with a compression ratio 9:1 (90%). The following code iterates over the compressed files, unzips them, and converts the CSV files to parquet format resulting in a total uncompressed size of just over 9GB. The parquet files are loaded into to year/month folder structure on disk.\n\n\nUnzip and convert CSV files to parquet (long run time)\n# `obs_data_files` is a list of files in `data_dir` that match the pattern \"zip\"\n# `queue` is a list of source files grouped by destination paths in `data_dir` \nqueue &lt;- { define(\n  data.table()\n  # Retrieve the paths of the source files\n  , from = dir(\n      path = getOption(\"params\")$ext_data_dir |&gt; paste0(\"/data files\")\n      , pattern = \"zip\"\n      , recursive = TRUE\n      , full.names = TRUE\n      )\n  # Extract the date from the file path\n  , date_str = stri_extract_last_regex(from, \"([0-9]{2}[[:punct:][[:space:]]]){2}[0-9]{2,4}\") |&gt;\n      stri_replace_all_regex(\"[[:punct:][[:space:]]]\", \"/\", vectorize_all = FALSE)\n  # Determine the date format\n  , fmts = map(date_str, \\(i) lubridate::guess_formats(i, orders = c(\"mdy\", \"dmy\", \"ymd\")) |&gt; first())\n  # Create the destination paths\n  , to = pmap_chr(list(from, date_str, fmts), \\(path, date_str, fmt){ \n      sprintf(\n        fmt = \"%s/parquet/%.0f/%s\"\n        , dirname(path) |&gt; dirname()\n        , lubridate::parse_date_time(date_str, orders = fmt) |&gt; lubridate::year()\n        , lubridate::parse_date_time(date_str, orders = fmt) |&gt; lubridate::month() |&gt; stri_pad_left(width = 2, pad = \"0\")\n        )\n    })\n  , clean = !duplicated(to)\n  # Remove unneeded columns\n  , ~from + to + clean\n  , .SD[order(to, !clean, from)]\n  )[] |&gt;\n  array_branch(margin = 1)\n}\n\n# If there are no parquet files in '`data_dir`/parquet', \n# unzip the files in `obs_data_files` and create container directories. \n# Files are in CSV format and will be converted to parquet format in parallel.\nif (!rlang::is_empty(queue) && {\n  reduce(queue, rbind)[, 2] |&gt; \n    unique() |&gt; \n    map_lgl(\\(i) dir(i, pattern = \"parquet\") |&gt; rlang::is_empty()) |&gt; \n    any()\n  }){\n  # Split the list of files in `data_dir` into n chunks for parallel processing\n  cl &lt;- parallelly::makeClusterPSOCK(4, autoStop = TRUE);\n  \n  # Export object `unzip.iter` to each cluster node\n  parallel::clusterExport(cl = cl, varlist = c(\"unzip.iter\"));\n  \n  tictoc::tic(glue::glue(\"[{Sys.time()}] Process Compressed Files\"));\n  process_log &lt;- { \n    parallel::parLapplyLB(cl = cl, X = queue, \\(x){\n      out &lt;- tryCatch(\n        unzip.iter(\n          from = x[1]\n           , to = x[2]\n           , clean = x[3] |&gt; trimws() |&gt; as.logical()\n           )\n        , error = \\(e){ \n            message(glue::glue(\"Error: {e} &lt;{x[1]}&gt;\"), sep = \"\\n\");\n            return(FALSE);\n          }\n        );\n      \n      gc();\n      out\n    })\n  }\n  tictoc::toc(log = TRUE);\n  \n  gc();\n  parallel::stopCluster(cl = cl)\n  \n  # Review potential errors ----\n    queue[map_lgl(process_log, \\(p) rlang::`%||%`(p, FALSE) == FALSE )] |&gt; \n    reduce(rbind) |&gt; \n    View()\n  \n  rm(queue, unzip.iter, cl);\n}\n\n# Retrieve data from the Arrow dataset\nobs_data &lt;- open_dataset(\n  sources = getOption(\"params\")$ext_data_dir |&gt; paste0(\"/data files/parquet\")\n  , hive_style = FALSE\n  , partitioning = c(\"Year\", \"Month\")\n  )\n\n\nAfter conversion of CMS zipped files to parquet format, and reading into the Arrow dataset, the data is ready for analysis. The number of rows and columns in the dataset are 302,720,873 x 287.\n\n\n\nData Dictionary\n\n\nA data dictionary is provided in PDF format. Using the PDF Tables service, the PDF was converted into CSV for easier processing:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This project explores the CMS COVID-19 Nursing Home Data as reported the week of 2023-11-05. Featured in this project is the use of R package arrow to read and query the data. Additional information about the data can be found in the linked provided above.\nInitially, the focus for this body of work is exploration and getting used to the R package arrow. One of my objectives for this project was to put to use my exposure to arrow and have an opportunity to work with a dataset larger than what I normally use at work or personally. The CMS COVID-19 dataset is a good candidate for this."
  },
  {
    "objectID": "CMS_C19_NH_Exploration.html",
    "href": "CMS_C19_NH_Exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "1. Initialize2. Load Existing Data3. Data Dictionary4. Data Exploration\n\n\nThis section executes global tasks that set up the workspace are executed:\nLoad Libraries\nThis code block sets global parameters for the session and loads an initial set of libraries. Custom libraries are hosted on Github at github.com/delriaan and include the following:\n\nbook.of.utilities\nbook.of.features\nbook.of.workflow\nsmart.data\nDBOE\nevent.vectors\narchitect\n\n\n\nNo required values set: exiting ..;\n\n\n\n\n\n\nLoad existing data and make local database connections\n# :: Database Connections ====\nif (!hasName(globalenv(), \"db_conns\")){\n  assign(\"db_conns\", list());\n  \n  db_conns$LOCAL &lt;- dbConnect(\n    drv = odbc::odbc()\n    , dsn = getOption(\"params\")$dsn\n    , UID = workflow$ez_pass$sql@username\n    , PWD = workflow$ez_pass$sql@key()\n    );\n  \n  duck_drv &lt;- duckdb();\n  db_conns$DUCK &lt;- dbConnect(drv = duck_drv)\n}\n\n# :: Database Metadata ====\ntic(\"$get.metadata()\");\ndbms.monitor &lt;- DBOE$new()$get.metadata(!!!db_conns[1], chatty = !TRUE);\ntoc(log = TRUE);\n\n\n$get.metadata(): 0.8 sec elapsed\n\n\nLoad existing data and make local database connections\ntic(\"Load Saved Data\")\nmessage(\"Load cached data\");\ncache_load(cache = caches, pattern = \"^(global|base|arch|temp|meta)\");\n\n\nNULL\n\n\nLoad existing data and make local database connections\ntoc(log = TRUE)\n\n\nLoad Saved Data: 0.06 sec elapsed\n\n\nLoad existing data and make local database connections\ncheck.env(.GlobalEnv);\n\n\n\n\nA data dictionary is provided in PDF format. Using the PDF Tables service, the PDF was converted into CSV for easier processing:\n\n\n\nSummary StatsTBD\n\n\n\n\nCode\nif (!\"global_obs_summary\" %in% caches$keys()){\n  # Read parquet files into the workspace\n  obs_data &lt;- open_dataset(\n    sources = getOption(\"params\")$ext_data_dir |&gt; paste0(\"/data files/parquet\")\n    , hive_style = FALSE\n    , partitioning = c(\"Year\", \"Month\")\n    );\n\n  if (!interactive()){ \n    stop(\"`obs_summary` not found in cache: code must be run interactively in order to generate.\")\n  }\n  \n  # Because the dataset is large, I used repeated sampling to generate a summary of \n  #   descriptive statistics for the dataset. This is done by sampling 100 observations\n  #   from the dataset 30 times. The results are then combined and summarized. \n  # Columns that are all NA or integer64 are ignored, and rows with less than 60% valid\n  #   values are also ignored.\n  obs_summary &lt;- map2(1:30, 100, slowly(\\(i, n){\n      # obs_data.query |&gt; \n      obs_data |&gt; \n        dplyr::slice_sample(n = n) |&gt;\n        dplyr::mutate(sample_sz = n, iteration = i) |&gt;\n        dplyr::collect() |&gt;\n        dplyr::distinct() |&gt;\n        dplyr::collect() |&gt;\n        setDT()\n      }, rate_delay(0.5))) |&gt; \n    rbindlist() |&gt;\n    discard(\n      # Ignore columns that are all NA or integer64\n      \\(i) bit64::is.integer64(i) | all(is.na(i))\n      ) |&gt;\n    setcolorder(c(\"Year\", \"Month\"));\n  \n  # The data is saved to a cache file for reuse in the project\n  cache_prep(obs_summary) |&gt; cache_save(caches);\n  \n  gc()\n} else {\n  cache_load(caches, \"obs_summary\");\n}\n\n\n\n\n\n\n\nShow above is a summary of descriptive statistics for a sample of observations from the dataset.\nThe sample size is 100 observations and the sample is drawn randomly 30 times. Only rows with greater than or equal to 60% valid values and columns that are not all NA or integer64 are shown.\nThe table is interactive and can be searched, sorted, and filtered."
  }
]