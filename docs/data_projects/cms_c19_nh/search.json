[
  {
    "objectID": "CMS_C19_NH_ETL.html",
    "href": "CMS_C19_NH_ETL.html",
    "title": "ETL and Data Engineering",
    "section": "",
    "text": "1. Initialize2. Load Existing Data3. Get External DataAppendix\n\n\nThis section executes global tasks that set up the workspace are executed:\nLoad Libraries\nThis code block sets global parameters for the session and loads an initial set of libraries. Custom libraries are hosted on Github at github.com/delriaan and include the following:\n\nbook.of.utilities\nbook.of.features\nbook.of.workflow\nsmart.data\nDBOE\nevent.vectors\narchitect\n\n\n\nNo required values set: exiting ..;\n\n\nExecute globalized initialization routines\n\n\nObjects Created\n\n.dsn_table : ODBC DSN entries read from Windows registry\ndb_conns : A list of DBI-compliant connections\n.calendar_keys : Makes it easier to subset the calendar object\n.new_data_date : Temporal marker for “new” records (meaningful for prediction runs which include incremental ETL handling)\n.calendar_filter : Date boundaries of .calendar_keys for use in T-SQL queries\n\n\n\nLoad existing data and make local database connections\n# :: Database Connections ====\nif (!hasName(globalenv(), \"db_conns\")){\n  assign(\"db_conns\", list());\n  \n  db_conns$LOCAL &lt;- dbConnect(\n    drv = odbc::odbc()\n    , dsn = getOption(\"params\")$dsn\n    , UID = workflow$ez_pass$sql@username\n    , PWD = workflow$ez_pass$sql@key()\n    );\n  \n  duck_drv &lt;- duckdb();\n  db_conns$DUCK &lt;- dbConnect(drv = duck_drv)\n}\n\n# :: Database Metadata ====\ntic(\"$get.metadata()\");\ndbms.monitor &lt;- DBOE$new()$get.metadata(!!!db_conns[1], chatty = !TRUE);\ntoc(log = TRUE);\n\n\n$get.metadata(): 0.35 sec elapsed\n\n\nLoad existing data and make local database connections\ntic(\"Load Saved Data\")\nmessage(\"Load cached data\");\ncache_load(cache = caches, pattern = \"^(global|base|arch|temp|meta)\");\ntoc(log = TRUE)\n\n\nLoad Saved Data: 0.06 sec elapsed\n\n\nLoad existing data and make local database connections\ncheck.env(.GlobalEnv);\n\n\n\n\nThe CMS data was downloaded as separate files for a total compressed size of over 10GB. Each file contained one or more CSV documents with a compression ratio 9:1 (90%). The following code iterates over the compressed files, unzips them, and converts the CSV files to parquet format resulting in a total uncompressed size of just over 9GB. The parquet files are loaded into to year/month folder structure on disk.\n\n\nUnzip and convert CSV files to parquet (long run time)\n# `obs_data_files` is a list of files in `data_dir` that match the pattern \"zip\"\n# `unzip.iter` is a function that iterates over `obs_data_files` and unzips each file to its parent directory\n# `queue` is a list of source files grouped by destination paths in `data_dir` \nqueue &lt;- { define(\n  data.table()\n  # Retrieve the paths of the source files\n  , from = dir(\n      path = getOption(\"params\")$ext_data_dir |&gt; paste0(\"/data files\")\n      , pattern = \"zip\"\n      , recursive = TRUE\n      , full.names = TRUE\n      )\n  # Extract the date from the file path\n  , date_str = stri_extract_last_regex(from, \"([0-9]{2}[[:punct:][[:space:]]]){2}[0-9]{2,4}\") |&gt;\n      stri_replace_all_regex(\"[[:punct:][[:space:]]]\", \"/\", vectorize_all = FALSE)\n  # Determine the date format\n  , fmts = map(date_str, \\(i) lubridate::guess_formats(i, orders = c(\"mdy\", \"dmy\", \"ymd\")) |&gt; first())\n  # Create the destination paths\n  , to = pmap_chr(list(from, date_str, fmts), \\(path, date_str, fmt){ \n      sprintf(\n        fmt = \"%s/parquet/%.0f/%s\"\n        , dirname(path) |&gt; dirname()\n        , lubridate::parse_date_time(date_str, orders = fmt) |&gt; lubridate::year()\n        , lubridate::parse_date_time(date_str, orders = fmt) |&gt; lubridate::month() |&gt; stri_pad_left(width = 2, pad = \"0\")\n        )\n    })\n  , clean = !duplicated(to)\n  # Remove unneeded columns\n  , ~from + to + clean\n  , .SD[order(to, !clean, from)]\n  )[] |&gt;\n  array_branch(margin = 1)\n}\n\nunzip.iter &lt;- \\(from, to, clean = FALSE){\n  require(magrittr)\n  # browser()\n  # Capture the parent directory of `from` ----\n  zip_dir &lt;- dirname(from);\n  \n  # Create the name of the CSV file ----\n  csv_file &lt;- (utils::unzip(zipfile = from, list = TRUE))[, 1] |&gt;\n        purrr::keep(\\(i) stringi::stri_detect_fixed(str = i, pattern = \"COVID\"));\n  \n  if (rlang::is_empty(csv_file)){ \n    return(FALSE) \n  } else { \n    csv_file &lt;- paste(zip_dir, csv_file, sep = \"/\");\n  }\n  \n  # browser()\n  \n  # Create the name of the parquet file ----\n  if (!dir.exists(to)){ dir.create(to, recursive = TRUE) }\n  if (clean){ rlang::inject(file.remove(!!!dir(to, pattern = \"parquet\", full.names = TRUE))) }\n  \n  parquet_file &lt;- paste(\n    to\n    , stringi::stri_replace_last_fixed(\n        str = csv_file\n        , pattern = \".csv\"\n        , replacement = \".parquet\"\n        , vectorize_all = FALSE\n        ) |&gt;\n      stringi::stri_split_fixed(\"/\", simplify = TRUE) %&gt;% \n      .[, ncol(.)] |&gt;\n      stringi::stri_trans_tolower() |&gt;\n      stringi::stri_replace_all_regex(\"[_[:space:]]\", \"-\", vectorize_all = FALSE)\n    , sep = \"/\"\n    );\n  \n  # Check if the parquet file already exists ----\n  if (file.exists(parquet_file)){\n    # If so, do nothing\n    message(glue::glue(\"{parquet_file} already exists: skipping ...\"), sep = \"\\n\");\n    return(NULL);\n  } else {\n    # Otherwise, unzip `from` to `exdir` ----\n    utils::unzip(zipfile = from, exdir = zip_dir, overwrite = !TRUE); \n    \n    # Send the user a message that `from` is being unzipped to `zip_dir` ----\n    cat(glue::glue(\"Unzipping {from} to {zip_dir}\"), sep = \"\\n\");\n  \n    if (rlang::is_empty(csv_file)){ \n      return() \n    } else {\n      # Choose the largest CSV file ----\n      csv_file &lt;- csv_file[(\\(m) m$size == max(m$size))(file.info(csv_file))];\n    \n      cat(glue::glue(\"Converting {csv_file} to {parquet_file}\"), sep = \"\\n\");\n      \n      # Convert the largest CSV file to parquet ----\n      arrow::read_csv_arrow(csv_file) |&gt;\n        arrow::write_parquet(parquet_file);\n      \n      # Check if the parquet file was successfully written ----\n      if (file.exists(parquet_file)){ \n        # Delete the unzipped CSV file\n        cat(glue::glue(\"Successfully wrote {parquet_file}\"), sep = \"\\n\");\n        unlink(csv_file);\n        cat(glue::glue(\"Deleted {csv_file}\"), sep = \"\\n\");\n        return(TRUE);\n      } else {\n        # Failed to write parquet file\n        cat(glue::glue(\"Failed to write {parquet_file}\"), sep = \"\\n\");\n        return(FALSE);\n      }\n    }\n  }\n}\n\n# If there are no parquet files in '`data_dir`/parquet', \n# unzip the files in `obs_data_files` and create container directories. \n# Files are in CSV format and will be converted to parquet format in parallel.\nif (!rlang::is_empty(queue) && {\n  reduce(queue, rbind)[, 2] |&gt; \n    unique() |&gt; \n    map_lgl(\\(i) dir(i, pattern = \"parquet\") |&gt; rlang::is_empty()) |&gt; \n    any()\n  }){\n  # Split the list of files in `data_dir` into n chunks for parallel processing\n  cl &lt;- parallelly::makeClusterPSOCK(4, autoStop = TRUE);\n  \n  # Export object `unzip.iter` to each cluster node\n  parallel::clusterExport(cl = cl, varlist = c(\"unzip.iter\"));\n  \n  tictoc::tic(glue::glue(\"[{Sys.time()}] Process Compressed Files\"));\n  process_log &lt;- { \n    parallel::parLapplyLB(cl = cl, X = queue, \\(x){\n      out &lt;- tryCatch(\n        unzip.iter(\n          from = x[1]\n           , to = x[2]\n           , clean = x[3] |&gt; trimws() |&gt; as.logical()\n           )\n        , error = \\(e){ \n            message(glue::glue(\"Error: {e} &lt;{x[1]}&gt;\"), sep = \"\\n\");\n            return(FALSE);\n          }\n        );\n      \n      gc();\n      out\n    })\n  }\n  tictoc::toc(log = TRUE);\n  \n  gc();\n  parallel::stopCluster(cl = cl)\n  \n  # Review potential errors ----\n    queue[map_lgl(process_log, \\(p) rlang::`%||%`(p, FALSE) == FALSE )] |&gt; \n    reduce(rbind) |&gt; \n    View()\n  \n  rm(queue, unzip.iter, cl);\n}\n\n# Retrieve data from the Arrow dataset\nobs_data &lt;- open_dataset(\n  sources = getOption(\"params\")$ext_data_dir |&gt; paste0(\"/data files/parquet\")\n  , hive_style = FALSE\n  , partitioning = c(\"Year\", \"Month\")\n  )\n\n\nAfter conversion of CMS zipped files to parquet format, and reading into the Arrow dataset, the data is ready for analysis. The number of rows and columns in the dataset are 302,720,873 x 287.\n\n\n\nData Dictionary\n\n\nA data dictionary is provided in PDF format. Using the PDF Tables service, the PDF was converted into CSV for easier processing:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This project explores the CMS COVID-19 Nursing Home Data as reported the week of 2023-11-05. Featured in this project is the use of R package arrow to read and query the data. Additional information about the data can be found in the linked provided above.\nInitially, the focus for this body of work is exploration and getting used to the R package arrow. One of my objectives for this project was to put to use my exposure to arrow and have an opportunity to work with a dataset larger than what I normally use at work or personally. The CMS COVID-19 dataset is a good candidate for this."
  }
]