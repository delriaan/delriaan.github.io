[
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "ReferencesTechnology and LibrariesProject Files and Directories\n\n\n\nText2vec: The text2vec API provides a simple, robust framework for natural-language processing (NLP). It is used for word embeddings and topic modeling. Functionality is wrapped by a custom R6 object that encapsulates predefined processing steps.\n\nGLoVe\nCollocations\n\nUDPipe\nKeras is used for fitting the classifier neural networks with a Tensorflow backend.\nCorpora\n\nKaggle\n[Epub]\n\nengoebus\nengfbv\neng-glw\nengnet\n\n\nDeep Learning with R, Second Edition\n\n\n\n\nLanguages\n\nR 4.3.2 or newer\nPython 3.12.0\n\n\n\nGitHub Libraries\n\nbook.of.workflow\nbook.of.utilities\nbook.of.features\narchitect\n\n\n\nCRAN Libraries\n\narrow, httr2, dplyr, docstring\ntext2vec, udpipe, keras, tensorflow\nstringi, textclean, lexicon, readtext\nparallelly, foreach, doParallel, future.callr\n\n\n\n\n(A link to the GitHub repository will be provided after some directory clean up …)\n\nFiles\ndevelopment_workflow.qmd: This is the main development document that executes code in sections. It calls the following code from the following:\n\n_initialize.R: Sourcing this file sets up the computing environment for the project.\n_functions.R: Sourcing this file creates the functions used in the project.\n_corpora.R: This file contains multiple “snippets” focused on reading and transforming the corpora used for NLP tasks and to create the neural network classifier.\n_keras_model_book.R: This file contains multiple “snippets” used to support the book classifier model training workflow.\n_keras_model_genre.R: This file contains multiple “snippets” used to support the genre classifier model training workflow.\n_web_corpora.R: This file contains multiple “snippets” used to retrieve web content for the purpose of using the genre and book classifiers to make inferences.\n_site_management.R: This file contains code related to rendering Quarto documents, creating the site content, and exporting files and directories to predefined endpoints for deployment.\n\n\n\nDirectories\n\ncorpora: This directory contains the corpora used for training the neural network classifiers.\nviz: This directory contains visualizations created during various parts of the development cycle (typically as checkpoints to interactively view results and make adjustments).\nmodel_runs: This directory contains the results of training the neural network classifiers (defined as a Tensorflow callback).\ntensorboard: This directory contains the logs for Tensorboard.\nsite_development: This directory contains the exported files and directories for deployment.\nlogs: This directory contains log files for the project.\nsaved_data: This directory contains saved data objects.\n\n/cache: Objects cached to disk via cachem\n/models: Saved models\n/workspace: Saved R workspace binaries"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prelude",
    "section": "",
    "text": "This project is the result of a multi-month learning journey beginning in December 2023. The focus was on natural-language processing (NLP) and the neural network architecture."
  },
  {
    "objectID": "index.html#the-goals",
    "href": "index.html#the-goals",
    "title": "Prelude",
    "section": "The Goals",
    "text": "The Goals\n\nGoal 1\nI was already familiar with NLP techniques thanks to projects at my employer; however, I’d never tried to do anything that involved neural networks. I had a conceptual knowledge of how they worked, but there is no substitute for using the technology directly.\n\n\nGoal 2\nI wasn’t interested in reproducing toy examples I might find on the Internet, so I decided to draw from what I already had sufficient interest and experience with that would provide a challenge: the Bible and current events. The idea was to explore the possibility of taking a modern-day article (news or opinion) and classifying it based on one of the genres and corresponding books from the Bible. The underlying assumptions were simple:\n\n\nThe Bible is a “a collection of sixty-six books composed and compiled over 2,000 years by forty authors on three continents.”; therefore, every category the human experience and expression can be found in its passages.\nNews articles and opinion pieces also reflect much of the same experiences and expressions. Also, today’s news is tomorrow’s history.\nThe artifacts of human life and society generate novelties over time, but the motivations and behaviors of people are nothing new: same game, different players."
  },
  {
    "objectID": "index.html#the-journey",
    "href": "index.html#the-journey",
    "title": "Prelude",
    "section": "The Journey",
    "text": "The Journey\n\nGo!Class “Equity”Functional PatternsThe PlungeReflections\n\n\nDecember 8th, 2023: I purchased the book Deep Learning with R, Second Edition, and began the journey, one step at a time. After several weeks of reading, setting up the computing environment, practicing simple tasks based on examples, and numerous cycles bouncing back and forth between, “I’m never going to get this!” and “My power level is over 9000!”, it was time to take the plunge and merge existing NLP knowledge with what I was learning about neural networks, Tensorflow, and Keras.\nI had an existing framework for the NLP portion of the project using text2vec. Part of this framework involved projecting the document-term matrix (DTM) onto the word embedding vector space and then concatenate the document-level topics distribution. The DTM, word embeddings, and topics are native text2vec constructs, so the challenge was to figure out a way to use what I already had in a Tensorflow-oriented framework.\nThis is where I spent time learning how to create a custom preprocessing layer. Already having R6 class experience, it wasn’t too much of a challenge to follow the reading material fro the book. The challenging part was to integrate a text vectorization layer with the document topics and embedding vectors I’d already derived (being new to this, I was only so willing to re-invent the wheel, so to speak). More on this later.\n\n\nI was not prepared for how much time and effort I would end up spending time on trying to deal with class imbalance across train/validation/test (TVT) data splits. Something like the following was desired:\n\n\nClass Proportions\n\n\nclass_Aclass_Bclass_C0.130.530.33\n\n\nTVT Split Proportions\n\n\ntrainvaltest0.70.150.15\n\n\nIdeal Class-TVT Proportions\n\n\ntrainvaltest0.0910.01950.01950.3710.07950.07950.2310.04950.0495\n\n\nHowever, the TVT split assignments were stochastic, so the ideal proportionality scenario was sometimes difficult to achieve, especially for genres with number of books over four or so. I had to come up with a method for achieving a class proportionality fidelity across TVT splits.\n\n\nI’m always looking for opportunities to use a function to encapsulate a parameterized workflow. Specifying layer definitions as repeating sets and encapsulating model configuration and training steps (e.g. defining callbacks, fit, and compile options; then fitting and evaluating the model) were two such scenarios. Find a parameterized pattern, functionalize, rinse and repeat.\nCreating a layer-set generator allowed me to pass the number of sets as a hyper-parameter to the model configuration function and was a huge time saver.\nIn addition to automating balanced TVT dataset creation and layer generation, another set of patterns that needed to be addressed focused on monitoring model fitting as well as managing created models. I had originally relied on TensorBoard for monitoring model fitting and comparing runs … and then I stopped, mainly because I wanted direct access to model evaluation stats and custom logging in a framework that supported easy automation, quick iteration, model ranking, and tagged model storage for easy retrieval.\nSometimes, tools can be helpful in the wrong direction, and there’s something to be said for having the freedom to learn first before settling on a set of tools or methods.\n\n\nIt was finally time to focus on inference on new data. I wanted to use dynamic, current sources of data, so what better place to turn to than the news.\nJust Browsin’\nI did not (and do not) have all of the sites that I wanted to pull from set up for web-scraping. Setting up a framework for capturing and transforming web responses can be abstracted only so much due to variations in how a site’s content is structured, but the general idea was to capture or perform the following:\n\nSend a request to the homepage URL to capture the links leading to the major sections of interest from the homepage (e.g., opinion, politics, law, etc.)\nSend requests for each of the section URLs and extract the article URLs from the responses.\nIterate over the set of article URLs for each section (in a way that wouldn’t get me throttled).\n\nThe Epoch Times is the first article source to be used. AP News is the next planned source. I want to have three to five sources in total, but I haven’t put any thought in to what they would possibly be at this time.\nThe Final Step\nOnce I had the content, all that was left to do was parse out the article text, prepare the text in the same way the training corpora was, load the models, and make the predictions (I’m obviously leaving out the gory details of finding and parsing the web content like I needed). The process took more time than I would have thought — but it was worth it.\n\n\nI cannot fully express how enriching and challenging this project has been. Having gone from no practical experience with neural-network architecture to building a set of models working in concert was immensely satisfying and worth every bit of frustration experienced along the way.\nI have a much greater appreciation for the effort required for these kinds of projects. Sourcing data, implementing tooling, experimentation and refinement, and all of the logistics of model creation are not tasks one can take lightly (though I’m sure experience makes the process itself easier to navigate).\nI’ll close with a few lessons learned and observations:\n\nTaking the time to produce good training data is paramount. I spent much more time on this than figuring out the model architecture.\nThe volume of data is not the only focus. Knowing how to extract the latent information content from the data during the feature-engineering and augmentation phase can be of great benefit when the volume of data is not large or is of limited variety. Taking the data at face-value could result in missing important features.\nMind how you initialize your layer weights … seriously. Earlier in my efforts to create the base genre classifier, I kept getting models that were consistently performing poorly. Part of the issue turned out to be using default weights initialization instead of thinking about the task at hand and specifying the initialization.\nI gained a better feel for the interactions among batch size, layer breadth and depth, regularization, dropout, normalization (layer vs. batch), and the number of possible classes for the output. Learning to interpret the fitting curves to make adjustments at various points in the processing pipeline was a related skill I picked up.\nIt’s always darkest before daybreak. Don’t give up in the face of seemingly stymied progress. Give yourself some grace, especially when learning something new. The goal isn’t to emulate performance at someone else’s level (real or perceived). The goal is to learn the material in the way you need and then build from there. The first project completed in a new domain will be rough around all the edges — and that’s just fine."
  },
  {
    "objectID": "index.html#current-state",
    "href": "index.html#current-state",
    "title": "Prelude",
    "section": "Current State",
    "text": "Current State\n\nNLPModelsAutomation\n\n\nThe genre-to-book mappings were taken from the Bible Corpus dataset hosted on Kaggle. Other book-to-genre mappings exist; however, the ones from the Kaggle dataset provide strict groupings at the book level.\nTopic Modeling\nLatent Dirichlet Allocation (LDA) topic modeling is handled by the text2vec API with the workflow wrapped in custom R6 class text2vec_workflow. I chose to incorporate document-topics as features because part of the goal was to capture differences in what each genre was about by means of their topic distributions within and across genres and their associated books. The reasoning is related to some key features of the text as a whole:\n\nThe presence of a metanarrative threading its way through each book.\nThematic topics show up to varying degrees and proportions across books while specific topics are more confined to people, places, and bracketed windows of history, many of which being historic in additional to historical (for example, the creation event, the global flood, specific Roman provincial rulers, etc.).\n\nThe number of topics was chosen (\\(n=50\\)) was somewhat influenced by the preceding considerations. I didn’t want to use \\(n=66\\) as it works against the idea of topics being meaningfully book-wise heterogeneous.\nEmbeddings\nEmbeddings creation is also handled by text2vec and are by word-tokens rather than sub-word tokens since this is not a text generative project and that the corpus is closed. The dimensionality of the vector space (\\(d=264\\)) was simply chosen based on what would actually converge given the corpora as well as having the goal of being able to represent richness of content sometimes represented in relatively small regions of the corpora.\nCollocations\nRounding out the text2vec roadshow, both embeddings and topics were enhanced with modeled collocations which greatly helped.\nUDPipe\nThe final NLP construct used for feature engineering was Universal Part-of-Speech (UPOS) profiling for each document: 1. Each document was tagged with a UPOS label using the UDPipe API. 2. UPOS frequency counts were tabulated for each document and collectively turned into a \\(n \\times k\\) array (\\(n\\) documents by \\(k\\) UPOS labels). 3. During the creation of TVT datasets, the array is scaled after the TVT splits are made.\nThe embeddings, topics, and UPOS profiles provide information about the text from different perspectives, all of which being used to create the feature space for modeling.\n\n\nInput Layer\nTo combine text vectorization with the existing embeddings and topics, a text vectorization layer was piped into a custom preprocessing layer responsible for tensor multiplication. The basic flow is like the following:\n\ntext vectorization: \\(\\text{DT}\\big\\{\\text{n_docs, m_tokens}\\big\\}_\\text{keras}\\)\nDocument topic distribution: \\(\\text{DP}\\big\\{\\text{n_docs, p_topics}\\big\\}:= \\text{DT}_\\text{text2vec}\\times\\text{PT}^\\top\\big\\{\\text{p_topics, m_tokens}\\big\\}_\\text{text2vec}\\)\nDocument embedding vectors: \\(\\text{DV}\\big\\{\\text{n_docs, embed_dim}\\big\\}:=\\text{DT}_\\text{keras}\\times\\text{TV}\\big\\{\\text{m_tokens, embed_dim}\\big\\}_\\text{text2vec}\\)\nProject topic vectors onto the embedding space: \\(\\text{PV}\\big\\{\\text{p_topics, embed_dim}\\big\\} := \\text{PT}_\\text{text2vec}\\times\\text{TV}_\\text{text2vec}\\)\nConcatenation: \\(\\text{DV}\\cup\\big\\{\\text{DP}\\times\\text{PV}\\big\\}\\cup\\text{AN}\\)\n\n\\(\\text{AN}\\) is the document-level UPOS annotation profile intended to encode differences in the composition of the types of words in context of each document (in this case, a single chapter). \\(\\text{DV}\\) and \\(\\text{DP}\\times\\text{PV}\\) have the same shape but encode different information relative to the same embedding space; namely what a document is about and to what extent.\nClassification Models\nRegarding the classifiers, the current set consists of a genre classifier, and six book classifiers. Yes, I could have just tried for a 66-way classifier model … … \\(\\text{but that wasn't the goal}\\) .Also, the problem space is inherently hierarchical with mutually exclusive paths. Returning inferences for genre-book mismatches obviously would not be not acceptable. As such, each of the book classifiers were trained from the genre classifier with genre-specific training data.\nA simple “if-then” workflow controls inference on new data: if the genre classifier (\\(G\\)) predicts the \\(i^{th}\\) genre, use book classifier \\(B_i\\) to predict the books. A summary of the models is found on the Results page.\n\n\nNearly all of the development workflow is automation-friendly (with the exception of retrieving the some of the corpora). Three major design features make this possible:\n\nThe use of a YAML-based configuration file containing settings and options that can be updated on the fly.\nQuarto documents that allow larger sections of code to be run in chunks as well as be outlined in the RStudio IDE.\nA simple “snippets” API I created which allows specially-formatted code to be executed when called either within the same document or called from another document. This functionality is provided by the book.of.workflow package."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results Exploration",
    "section": "",
    "text": "Bible Tree MapPredictions\n\n\nArranged by Genre and Book, the following treemap show the space for which models were created:\nView as webpage\n \n\n\n Note:   [show] \n\nThe articles providing the content below were downloaded and staged for demonstration purposes. A future version of this page will allow for providing a URL and making on-demand inference.\nThe numeric values appearing above the article text represent proportionality relative to the maximum probability class. Thresholds were set so that only class prediction proportionality above a threshold of 0.85 (for genre and book classifications) were returned.\nDo not take the results too seriously. Some predictions will “feel” just right, and others will simply “miss the mark”."
  }
]